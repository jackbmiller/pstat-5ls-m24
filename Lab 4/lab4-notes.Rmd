---
title: "PSTAT 5LS Lab 4 Notes"
author: "Your Name Here"
date: "August 22, 2024"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stats250sbi)
```

# Lab Notes

Use this place to take any notes during your lab section.

- Type any notes here
- Add new notes by using the dash `-` to create a list

# Lab Tutorial

## Another Simulation Example

According to the American Pet Products Association, prior to the pandemic, 67% of all U.S. households had pets. Many people have speculated that the rate of pet ownership changed during the pandemic. Recently, a group of veterinarians surveyed a random sample of 480 U.S. households and found that 336 had at least one pet.

Is there evidence to support the claim that the rate of pet ownership differs from the 67% before the pandemic?

Our hypotheses to test if there is a difference are 
$$H_0: p = 0.67 \text{ and } H_A: p \neq 0.67$$

## Setting Up the Simulation

What are the elements of the simulation?

|                   | **Assuming the chance model...** |
|-------------------|----------------------------------|
| One draw          | *replace this with your answer* |
| Blue poker chip   | *replace this with your answer* |
| Yellow poker chip | *replace this with your answer* |
| Chance of blue    | *replace this with your answer* |
| One repetition    | *replace this with your answer* |

## Running the Simulation

Recall that we need to set a seed before running a simulation so that we can talk about the results as a class (it will be helpful for us to have the same results). Let's arbitrarily set the seed to 734. Run this in the `setSeed` code chunk below.

```{r setSeed, echo = FALSE}
set.seed(734)
```

## Running the Simulation

Then let's run the simulation 500 times. Make sure to enter values for:

- `chanceSuccess` (what we assume $p$ is when the null hypothesis is true)
- `numDraws` (this will equal the sample size)
- `numRepetitions` (the number of times you want to run the simulation so you can get a sense of the distribution of $\hat p$ when the null hypothesis is true)

```{r sim1}
sim1 <- simulate_chance_model(chanceSuccess =  , 
                              numDraws =  , 
                              numRepetitions =  )
```

`sim1` should now be in your Environment pane.

## Histogram of Simulation Results

The sample statistic is $\hat p = \frac{336}{480} = 0.70$, so let's put a line in the histogram to indicate our observed statistic so that we can get a sense about how unusual it it to get $\hat p = 0.70$ when $p = 0.67$. 

We have a two-sided hypothesis test, so it would have been just as unusual to see a sample proportion of 0.64, so we will put a line in the histogram to indicate this as well.

## Histogram of Simulation Results

Run the following `histSimulate` code chunk to create this histogram.

```{r histSimulate, echo = FALSE}
hist(sim1, 
     main = "Histogram of 500 Simulation Results",
     xlab = "Simulated proportion of U.S households with at least one pet")
abline(v = 0.70, col = "red")
abline(v = 0.64, col = "red")
```

## Estimated p-value

The estimated p-value is the sum of the number of simulations that are as weird as or weirder than we observed (so, 0.70 or larger) *and* the sum of the simulations that are as weird or weirder on the other side (at 0.64 or less). 

Run the `pvalue_sim1` code chunk to get the estimated p-value for this test.

```{r pvalue_sim1, echo = TRUE}
total <- sum(sim1 >= 0.70) + sum(sim1 <= 0.64)
estimated_pvalue <- total/500
estimated_pvalue

```

Our estimated p-value is *replace this with your answer*.

## Conclusion
After running the simulation 500 times, assuming that the chance of success was indeed 0.67, the probability that we would get an observed sample proportion of 336/480 or more extreme was computed to be 0.156.

The p-value of 0.156 is larger than even a significance level of 0.10. We have very little to support the claim that the rate of pet ownership differs from the 67% before the pandemic.

## But We Observed a Different Result?!
You might have asked yourself, "But wait, why do we have little to no evidence to support the claim? I did in fact observe a rate different than 67% in my sample (I observed 70%, in fact). Isn't a 3% difference strong evidence?"

This is where the normal theory can help us understand that the difference of 3% between the $\hat{p}$ and the assumed $p$ was not enough evidence.

## Histogram from Simulation
It turns out, the histogram that we created using simulation is in fact approximately normal. 

We talked about how the distribution is centered at $H_0$, the chance of success. Thus we can assume $\mu = 0.67$ in our example about pet ownership prior to the pandemic.

To find the standard error, which is an estimate of the standard deviation for our sample statistic $\hat p$, we will use the following formula:

$$\sqrt{\frac{p_0(1-p_0)}{n}}$$
For the example about pet ownership, let's compute the standard error, $SE(\hat{p})$.

## Histogram from Simulation
The standard error for $\hat p$ can be found with the formula 
$$\sqrt{\frac{p_0(1-p_0)}{n}}$$

Let's have R compute this for us. Enter values for `p_0` and `n` in the `computeSE` chunk below and then run the chunk to calculate the SE.

```{r, computeSE}
# Run this code chunk to compute the standard error for the normal distribution
p_0 <- 
n <- 
SE <- sqrt(p_0*(1 - p_0)/n)
SE 
```

Thus, the sampling distribution of $\hat p$ in the pet ownership example will be approximately N(________, ________).

## Normal Distribution for Our Example

Let's take a look at the approximate normal distribution for our pet ownership example. Recall that, when the null hypothesis is true, the mean is **0.67**, the standard error is **0.021**, and we want to shade **beyond** the values of 0.64 and 0.7. Run the `normalDistCode` chunk below.

```{r normalDistGraph, echo = TRUE}
plot_norm(mean = 0.67, 
          sd = 0.021, 
          shadeValues = c(0.64, 0.7), 
          direction = "beyond", 
          col.shade = "darkgreen")
```

## There Is in Fact Insufficient Evidence
Now that we see that the standard deviation is 0.021, or 2.1%, we can start to understand why observing a rate that was only 3% different from the assumed mean is not all that rare. 

Looking at the normal distribution plot, we can observe that 0.7 is only between 1 and 2 standard deviations above the mean. Again, not all that rare.

## Another Way to Calculate the p-value
The `stats250sbi` package we have been using also contains a function called `prop_test()` that will calculate the value of the test statistic and the p-value for us. 

You will need to send the function the following arguments:

- `x`: the number of "successes" in the sample
- `n`: the sample size
- `p`: the hypothesized population proportion
- `alternative`: where to shade (`"two.sided"`, `"less"`, `"greater"`)
- `conf.level` (optional): used to get a confidence interval (we will use this later)

## Using `prop_test` for Our Pet Ownership Example

We are testing $H_0: p = 0.67 \text{ and } H_A: p \neq 0.67$. You will need to enter values for 

- `x` the number of successes (here the number of households with a pet)
- `n` the sample size
- `p` the value we assume $p$ to be when $H_0$ is true
- `alternative` the direction of $H_A$ (`"two.sided"`, `"less"`, `"greater"` -- make sure the direction is in quotes)

in the `prop_test_pet` chunk below before you run it.

```{r prop_test_pet, echo = TRUE}
prop_test(x =  , 
          n =  , 
          p =  , 
          alternative =  )
```

## The `pnorm()` Function
The p-value from `prop_test()` is pretty close to the p-value from our simulation. 

We can also compute the p-value using the `pnorm()` function. Recall the arguments you need to send to `pnorm()`:

- `q`: the quantile (value on the axis) for the normal distribution
- `mean`: the mean of the normal distribution ($\mu$)
- `sd`: the standard deviation of the normal distribution ($\sigma$)
- `lower.tail`: set to \textcolor{blue}{`TRUE`} initially, signifying that R will compute the probability **to the \textcolor{blue}{LEFT}** of `q`; if you would like R to compute the probability *to the \textcolor{red}{right}* of `q`, set `lower.tail` to \textcolor{red}{FALSE}

## Computing the p-value for the Simulation Example
Let's compute the approximate p-value using the normal distribution for our pet ownership example. Recall that the mean is **0.67**, and the standard deviation is **0.021**.

Because the normal distribution is **symmetric** about the mean, we can find the probability of observing 0.7 or greater, then **double it**. 

Or, we can find the probability of observing 0.64 or less, then **double it**.

Or, we can find the probability of observing 0.70 or greater, then the probability of observing 0.64 or less, and add the result.

Since 0.7 is the $\hat{p}$ for our sample, we will utilize the first option in the `pvalue` code chunk.

## Computing the p-value for the Simulation Example

Run the `pvalue` code chunk below to get the p-value from the normal distribution.

```{r pvalue}
2 * pnorm(q = 0.7, 
      mean = 0.67, 
      sd = 0.021, 
      lower.tail = FALSE)
```

## Comparing the Various p-values
To recap, we have *three* ways to compute the p-value for a one proportion hypothesis test:

1. Create a vector of simulated proportions using `simulate_chance_model()`, then using the `sum()` function to count the number of observations at or beyond the sample proportion divided by the number of observations 
1. Use the `prop_test()` function (which uses normal theory) by sending the number of successes observed, the sample size, the value of $H_0$, and the direction of the alternative hypothesis 
1. Compute the $\mu$ and $\sigma$ for the approximate normal distribution. Use the `pnorm()` function by sending the value of the sample proportion, $\mu$, $\sigma$, and the direction of the probability 

Each of these will produce a slightly different result. **No need to worry about how close the values should be, or which value is "best".**

## Comparing the Various p-values
\scriptsize
```{r comparepValue}
sum(sim1 <= 0.64) / 500 + sum(sim1 >= 0.7) / 500
prop_test(x = 336, n = 480, p = 0.67)
pnorm(q = 0.7, mean = 0.67, sd = 0.021, lower.tail = FALSE) * 2
```
\normalsize

## Comparing the Various p-values
1. The simulation is the most accurate, because it is computing the p-value with simulated values.
1. `prop_test()` and `pnorm()` will lose some precision due to utilizing the normal approximation. This loss of precision should not affect our results.
1. `pnorm()` will lose some precision if we round the standard deviation to 3 decimal places. This loss of precision should not affect our results.

## Using `prop_test()` to Find Confidence Intervals
The output from `prop_test()` also provides a confidence interval for the population proportion. The default confidence level is 0.95 for a 95% confidence interval. The 95% confidence interval for this example is (0.659, 0.741).

```{r prop_test_pet2, echo = FALSE}
prop_test(x = 336, 
          n = 480, 
          p = 0.67, 
          alternative = "two.sided")
```

We didn't need to specify that we wanted a 95% confidence interval because that is the default.

## Using `prop_test()` to Find Confidence Intervals
We can change the confidence level to a level other than 95% by adding the argument `conf.level` to the `prop_test()` function. 

Also, **if we just need to calculate a confidence interval** we don't have a hypothesized value of $p$ and we don't have an alternative hypothesis, so we drop the `p` and `alternative` arguments from `prop_test`.

## Using `prop_test()` to Find Confidence Intervals
In the `prop_test_pet3` chunk, set `conf.level` to 0.98.

```{r prop_test_pet3, echo = TRUE}
prop_test(x = 336, 
          n = 480, 
          alternative = "two.sided", 
          conf.level = 0.98)
```

A 98% confidence interval is (0.651, 0.749).

## Using `prop_test()` to Find Confidence Intervals
**Caution:** To get a two-sided confidence interval, the `alternative` argument *must* be set to `two.sided`. If it isn't, you will get a *confidence bound*. 

Note that `two.sided` is the default for `prop_test()`, so if you just want a confidence interval you can leave the `alternative` argument off. 

Confidence bounds can be useful when we have one-sided hypothesis tests, but we will leave them to your later statistics courses. 

## One More Example

X, the social media platform formerly known as Twitter, saw a name change and policy changes once it was under new ownership. Shortly after the name change, seventy-five percent (75%) of all X users said that they planned to keep using X in 2024. Researchers at X would like to know if this rate differs for Millennials, those born between 1981 and 1996. A random sample of 100 Millennial users of X revealed that 65% planned to keep using X in 2024.

## What Would We Expect to See in the Sample?

If the rate of current Millennial users of X is the same as the rate for all X users, how many Millennial users of X from the sample of 100 would we expect to say that they planned to keep using X in 2024?

## Incorrect Alternative

\small
The researchers at X wanted to know if the rate of Millennials who planned to continue using X in 2024 differs from the rate of all users of X. The researcher in charge accidentally ran a one-sided hypothesis test and got the following output

```{r incorrect_alternative}
prop_test(x = 65, n = 100, p = 0.75, alternative = "less")
```

## Correcting the Mistake

The value of the test statistic for the incorrect alternative was $z = -2.309$. What should the test statistic be for the correct two-sided test?

- It should be half as much; that is, $z = -1.155$.
- It should be positive instead of negative; that is, $z = 2.309$.
- It should be the same; that is, $z = -2.309$.
- It should be twice as much; that is, $z = -4.618$.

## Correcting the Mistake

The p-value for the incorrect alternative was 0.01046. What should the p-value be for the correct two-sided test?

- It should be half as much; that is, p-value = 0.00523.
- It should be negative instead of positive; that is, p-value = $-0.01046$.
- It should be the same; that is, p-value = 0.01046.
- It should be twice as much; that is, p-value = 0.02092.

# Code Cheat Sheet

## `set.seed(seed)`
Sets the "seed" of R's random number generator. After setting the seed, the sequence of random numbers R will produce is entirely determined/predictable. This is useful for ensuring you get the same results whenever you knit your code.

- `seed` is an integer. The seed you want to set.

## `simulate_chance_model(chanceSuccess, numDraws, numRepetitions)`

- `chanceSuccess`: a number between 0 and 1 represending the probability of observing a "success"
- `numDraws`: the number of times to draw a poker chip from the bag needed to coplete one repetition of the simulation
- `numRepetitions`: the number of times to repeat the simulation process

## `abline(linear_model_name)`

- Will plot the line found in `linear_model_name`
- Use `v = value` to print a vertical line

## `plot_norm()` 

- `mean`: the mean of the normal distribution you'd like to draw ($\mu$)
- `sd`: the standard deviation or standard error of the normal distribution you'd like to draw ($\sigma$ or $\sqrt{\frac{p_0(1-p_0)}{n}}$, respectively)
- `shadeValues` (optional): either a number or a vector of two numbers (using `c()`) that are the boundaries of the region you'd like to shade.
- `direction`: where to shade (`"less"`, `"greater"`, `"between"`, or `"beyond"`)
- `col.shade`: the color to use when shading
- any other graphical parameters you want to use to control the appearance of the plot (like `main`, etc.)

## `prop_test(x, n, p, alternative)`

- `x`: the number of successes observed in the study
- `n`: the sample size of the study
- `p`: the value of the null hypothesis
- `alternative`: the direction of the alternative hypothesis; choices are `"two.sided"` (default), `"greater"`, or `"less"`

## `pnorm(q, mean, sd, lower.tail = TRUE)`

- `q`: the x-axis value you want to find an area related to
- `mean`: the mean of the normal distribution, defaults to 0
- `sd`: the standard deviation of the normal distribution, defaults to 1
- `lower.tail` determines whether `pnorm()` finds the area to the left or right of `q`. If `lower.tail` is set to \textcolor{blue}{`TRUE`} (the default), it shades to the \textcolor{blue}{LEFT} If `lower.tail` is set to \textcolor{red}{`FALSE`}, it shades to the \textcolor{red}{RIGHT}.

## Important plotting arguments

### `main = "Title of Your Graph in Double Quotes"`
- graph title that must be inside a set of double quotes

### `xlab = "x-axis Label of Your Graph in Double Quotes"`
- the x- (horizontal) axis label that must be inside a set of double quotes

### `ylab = "y-axis Label of Your Graph in Double Quotes"`
- the y- (vertical) axis label that must be inside a set of double quotes
